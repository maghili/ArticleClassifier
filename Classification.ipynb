{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of articles based on their content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completely analyzing the text data we need to call in a few packages in python that come in handy for manipulation of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['completed', 'grg.sh', 'grg_dates.txt', 'grg_abstract.txt', '.DS_Store', 'archive', 'Classification.ipynb', 'grgaddresses.txt', '.ipynb_checkpoints', '_DS_Store', 'grg_titles.txt'] ['papercollector01.sh', 'papercollector04.sh', 'dates.txt', 'contents.txt', 'title.txt', 'addresses.txt', 'papercollector03.sh', 'papercollector02.sh']\n"
     ]
    }
   ],
   "source": [
    "print os.listdir('./'), os.listdir('./archive/epjc_archieve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the required documentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/miremadaghili/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/miremadaghili/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/miremadaghili/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the data that we collected from the <strong><em>General Relativity and Gravittation Journal</em></strong>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('grg_abstract.txt', 'r') as grg_f:\n",
    "    grg_abstracts = [line.decode('utf-8').split('\\n')[0] for line in grg_f if len(line)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'A simple unified closed form derivation of the non-linearities of the Einstein, Yang-Mills and spinless (e.g. chiral) meson systems is given. For the first two, the non-linearities are required by locality and consistency; in all cases, they are determined by the conserved currents associated with the initial (linear) gauge invariance of the first kind. Use of first-order formalism leads uniformly to a simple cubic self-interaction.',\n",
       " u'In an ingenious way rotation (but no angular momentum) has been introduced in the case of spherical symmetry by Einstein, who has considered a stationary cluster of particles moving freely under the influence of the gravitational field produced by all of them together. The aim of the present work is to extend his idea to the non-static case, and it seems that under some circumstances instead of an indefinite gravitational collapse there is a minimum of the volume and a bouncing back.',\n",
       " u'The Friedmann and Kantowski-Sachs models of the universe are embedded in eight-dimensional pseudo-Euclidean spaces in such a way, that their boundaries have the same structure as Geroch\\'s<em class=\"EmphasisTypeItalic \">g</em>-boundary.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grg_abstracts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we load the data that we previousely obtained from the <strong><em>European Physical Journal C</em></strong>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./archive/epjc_archieve/contents.txt', 'r') as epjc_f:\n",
    "    epjc_abstracts = [line.decode('utf-8').split('\\n')[0] for line in epjc_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'\"A framework associating quantum cosmological boundary conditions to minisuperspace hidden symmetries has been introduced in Jalalzadeh and Moniz (Phys Rev D 89:083504, 2014). The scope of the application was, notwithstanding the novelty, restrictive because it lacked a discussion involving realistic matter fields. Therefore, in the present letter, we extend the framework scope to encompass elements from a scalar\\u2013tensor theory in the presence of a cosmological constant. More precisely, it is shown that hidden minisuperspace symmetries present in a pre-big bang model suggest a process from which boundary conditions can be selected.\"',\n",
       " u'\"A detailed study of top-quark polarizations and charge asymmetries, induced by top-squark-pair production at the LHC and the subsequent decays , is performed within the effective description of squark interactions, which includes the effective Yukawa couplings and another logarithmic term encoding the supersymmetry breaking. This effective approach is more suitable for its introduction into Monte-Carlo simulations and we make use of its implementation in MadGraph in order to investigate the possibilities of the charge asymmetry , measured at the LHC and consistent with SM expectations, to discriminate between different SUSY scenarios and analyze the implications of these scenarios in the top polarizations and related observables.\"',\n",
       " u'\"In this work it has been developed a new approach to study the stability of a system composed by an ELKO field interacting with dark matter, which could give some contribution in order to alleviate the cosmic coincidence problem. It is assumed that the potential which characterizes the ELKO field is not specified, but it is related to a constant parameter . The strength of the interaction between matter and ELKO field is characterized by a constant parameter and it is also assumed that both ELKO field and matter energy density are related to their pressures by equations of state parameters and , respectively. The system of equations is analyzed by a dynamical system approach. It has been found the conditions of stability between the parameters and in order to have stable fixed points for the system for different values of the equation of state parameters and , and the results are presented in form of tables. The possibility of decay of the ELKO field into dark matter or vice versa can be read directly from the tables, since the parameters and satisfy some inequalities. It allows us to constrain the potential assuming that we have a stable system for different interactions terms between the ELKO field and dark matter. The cosmic coincidence problem can be alleviated for some specific relations between the parameters of the model.\"']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epjc_abstracts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start processing the text, we look at the two classes and their distribution to see if the classes are actually skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grg percentage:  0.584477930391\n",
      "epjc percentage:  0.415522069609\n"
     ]
    }
   ],
   "source": [
    "print 'grg percentage: ', len(grg_abstracts)*1./(len(grg_abstracts)+len(epjc_abstracts))\n",
    "print 'epjc percentage: ',len(epjc_abstracts)*1./(len(grg_abstracts)+len(epjc_abstracts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class weights are actually not too far from eachother, therefore, at the first step we will not try to balance the data and we go streight for tokenizing the texts.<br>\n",
    "For the next step we will create a list of words from the abstracts, and using the stoppingwords method in nltk, we will remove the commonly used words that will not help our model such as \"is\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change stemming to reduced as well\n",
    "total_abstracts = grg_abstracts+epjc_abstracts\n",
    "stem_lst = []\n",
    "lemma_lst = []\n",
    "for abstract in total_abstracts:\n",
    "    words = word_tokenize(abstract)\n",
    "    reduced = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            reduced.append(word)\n",
    "    lemma_lst += reduced\n",
    "    stem_lst += reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that Lemmatizing ans stemming have their own benefit and each one works better in different situations. Therefore, we useboth of them and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Let us use stemming on the text of the abstracts and create the stemming feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "stem_lst = [ps.stem(w.lower()) for w in stem_lst]\n",
    "set_of_unique_words = set(stem_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the fequency of different words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_all_words = nltk.FreqDist(stem_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18851\n"
     ]
    }
   ],
   "source": [
    "print len(stem_all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use only the top 5000 features to save some computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_word_features = stem_all_words.keys()[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a function that gets the text and the featurelins and returns a dictionary shows which features the text has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_finder(text, features):\n",
    "    '''This function is creating a feature list'''\n",
    "    text_words = word_tokenize(text)\n",
    "    lst = {}\n",
    "    for word in features:\n",
    "        lst[word] = (word in text_words)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_Data = [feature_finder(abstract, stem_word_features) for abstract in total_abstracts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the labels. We are going to use 1 for <strong>GRG</strong> articles and 0 for <strong>EPJC</strong> articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1]*len(grg_abstracts)+[0]*len(epjc_abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing the data\n",
    "We devide the data into training ans testing sets to be able to check the performance of the model, and make sure our model does not overfit or underfit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(stem_Data, labels,\n",
    "                                                    test_size = .1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_train = [(X_train[i],y_train[i]) for i in range(len(X_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(stem_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayesClassifier accuracy wit stemming: 98.0\n"
     ]
    }
   ],
   "source": [
    "stem_test = [(X_test[i], y_test[i]) for i in range(len(X_test))]\n",
    "print 'NaiveBayesClassifier accuracy wit stemming:', nltk.classify.accuracy(classifier, stem_test)*100"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We obtain a very high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "\n",
    "Heer we use the lemmatizing approach to look at the efficiency of the Naive Bayes Classifier. We follow the exact same procedure except with the Lemmatized feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma_lst = [lemmatizer.lemmatize(w.lower()) for w in lemma_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_all_words = nltk.FreqDist(lemma_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22863\n"
     ]
    }
   ],
   "source": [
    "print len(lemmatize_all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too many features. As before we are only going to use the top 5000 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_word_features = lemmatize_all_words.keys()[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lemma_data = [feature_finder(abstract, lemmatize_word_features) for abstract in total_abstracts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(stem_Data, labels,\n",
    "                                                    test_size = .1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_train = [(X_train[i], y_train[i]) for i in range(len(X_train))]\n",
    "lemma_test = [(X_test[i], y_test[i]) for i in range(len(X_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_classifier = nltk.NaiveBayesClassifier.train(lemma_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayesClassifier accuracy with lemmatization:  98.0\n"
     ]
    }
   ],
   "source": [
    "print 'NaiveBayesClassifier accuracy with lemmatization: ', nltk.classify.accuracy(lemma_classifier, lemma_test)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before. Very high accuracy in classification of the articles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
